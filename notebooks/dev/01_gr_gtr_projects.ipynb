{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/Users/grichardson/mysqldb_team.config'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_engine(config_path, database=\"production\", **engine_kwargs):\n",
    "    '''Get a SQL alchemy engine from config'''\n",
    "    cp = ConfigParser()\n",
    "    cp.read(config_path)\n",
    "    cp = cp[\"client\"]\n",
    "    url = URL(drivername=\"mysql+pymysql\", database=database,\n",
    "              username=cp[\"user\"], host=cp[\"host\"], password=cp[\"password\"])\n",
    "    return create_engine(url, **engine_kwargs)\n",
    "\n",
    "\n",
    "chunksize = 1000\n",
    "\n",
    "engine = get_engine(config_path)\n",
    "engine.execution_options(stream_results=True)\n",
    "\n",
    "# Projects\n",
    "request = pd.read_sql_table('gtr_projects', engine, chunksize=chunksize)\n",
    "projects = pd.concat(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(projects.shape)\n",
    "projects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects.to_csv(f'{data_path}/raw/gtr_projects.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = projects.profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file(f'{project_dir}/reports/eda/gtr_projects_profile.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "projects.groupby('leadFunder')['id'].count().plot.bar(ax=ax)\n",
    "ax.set_ylabel('Count')\n",
    "plt.savefig(f'{project_dir}/reports/eda/gtr_projects_leadFunder_count_bar.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_lengths = projects['abstractText'].str.len()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(abstract_lengths, bins=50)\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_xlabel('Abstract Length')\n",
    "plt.savefig(f'{project_dir}/reports/eda/gtr_projects_abstractText_length_hist.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks like there's some kind of word limit.\n",
    "- Can also see some peaks.\n",
    "- Lengths may vary depending on funder. Let's look at this in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_lengths.hist??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_lengths.hist(\n",
    "    by=projects['leadFunder'], density='normed',\n",
    "    cumulative=True, sharex=True, sharey=True, histtype='step', bins=100, linewidth=2\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_lengths[projects['leadFunder'] == 'ESRC'].hist(bins=np.linspace(0, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there might be some boiler plate text. Let's have a look at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for funder in projects['leadFunder'].unique():\n",
    "    print(f'========={funder}==========')\n",
    "    texts = projects[projects['leadFunder'] == funder]['abstractText'].value_counts()[:10]\n",
    "    print(texts[texts.to_frame().index.str.len() > 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to drop duplicates on `abstractText`, but we also need to get rid of rows with some specific abstract texts. We will create a custom data set with these and store it in `data/aux`.\n",
    "\n",
    "We will drop any row containing boiler plate abstracts:\n",
    "\n",
    "> Doctoral Training Partnerships: a range of postgraduate training is funded by the Research Councils. For information on current funding routes, see the common terminology at www.rcuk.ac.uk/StudentshipTerminology. Training grants may be to one organisation or to a consortia of research organisations. This portal will show the lead organisation only.\n",
    "\n",
    "> Abstracts are not currently available in GtR for all funded research. This is normally because the abstract was not required at the time of proposal submission, but may be because it included sensitive information such as personal details.\n",
    "\n",
    "We will also remove any abstracts with a length < 140 characters.\n",
    "\n",
    "We will also drop duplicates of anything left over (retaining one copy).\n",
    "\n",
    "There are no rows with missing abstracts or funders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "Return a dataframe that only has the columns we need, and with rows dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{data_path}/aux/gtr_projects_abstractText_drop.txt', 'r') as f:\n",
    "    abstract_texts_drop = f.read().splitlines()\n",
    "\n",
    "def clean(df):\n",
    "    df = df[['abstractText', 'leadFunder']]\n",
    "    df = df.drop_duplicates(subset='abstractText')\n",
    "    df = df[df['abstractText'].str.len() > 100]\n",
    "    df = df[~df['abstractText'].isin(abstract_texts_drop)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_clean = clean(projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(projects_clean.shape)\n",
    "projects_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english') +\n",
    "                 list(string.punctuation) +\n",
    "                 ['\\\\n'] + ['quot'])\n",
    "\n",
    "regex_str = [\"http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|\"\n",
    "             r\"[!*\\(\\),](?:%[0-9a-f][0-9a-f]))+\",\n",
    "             r\"(?:\\w+-\\w+){2}\",\n",
    "             r\"(?:\\w+-\\w+)\",\n",
    "             r\"(?:\\\\\\+n+)\",\n",
    "             r\"(?:@[\\w_]+)\",\n",
    "             \"<[^>]+>\",\n",
    "             r\"(?:\\w+'\\w)\",\n",
    "             r\"(?:[\\w_]+)\",\n",
    "             r\"(?:\\S)\"\n",
    "             ]\n",
    "\n",
    "# Create the tokenizer which will be case insensitive and will ignore space.\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')',\n",
    "                       re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "\n",
    "def tokenize_document(text, min_length=3, flatten=False):\n",
    "    \"\"\"Preprocess a whole raw document.\n",
    "    Args:\n",
    "        text (str): Raw string of text.\n",
    "        min_length (int, optional): Minimum token length\n",
    "        flatten (bool): Whether to flatten out sentences\n",
    "    Returns:\n",
    "        List: preprocessed and tokenized documents\n",
    "    #UTILS\n",
    "    \"\"\"\n",
    "    text = [clean_and_tokenize(sentence, min_length)\n",
    "            for sentence in nltk.sent_tokenize(text)]\n",
    "    if flatten:\n",
    "        return list(chain(*text))\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "def clean_and_tokenize(text, min_length):\n",
    "    \"\"\"Preprocess a raw string/sentence of text.\n",
    "    Args:\n",
    "        text (str): Raw string of text.\n",
    "        min_length (int): Minimum token length\n",
    "    Returns:\n",
    "        list of str: Preprocessed tokens.\n",
    "    #UTILS\n",
    "    \"\"\"\n",
    "\n",
    "    # Find tokens and lowercase\n",
    "    tokens = tokens_re.findall(text)\n",
    "    _tokens = [t.lower() for t in tokens]\n",
    "    # Remove short tokens, stop words, tokens with digits, non-ascii chars\n",
    "    filtered_tokens = [token.replace('-', '_') for token in _tokens\n",
    "                       if len(token) >= min_length\n",
    "                       and token not in stop_words\n",
    "                       and not any(x in token for x in string.digits)\n",
    "                       and any(x in token for x in string.ascii_lowercase)]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = 3\n",
    "tokenized = projects_clean['abstractText'].apply(tokenize_document, min_length=min_length, flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
